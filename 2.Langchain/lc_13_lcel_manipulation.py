import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableLambda, RunnablePick
from langchain_core.output_parsers import StrOutputParser

# ============================================================================
# LCEL Data Manipulation: ì²´ì¸ ë‚´ë¶€ì˜ ë°ì´í„° íë¦„ ì œì–´í•˜ê¸°
# ============================================================================
# ì™œ í•„ìš”í•œê°€?
# 1. Chainì€ ê¸°ë³¸ì ìœ¼ë¡œ ì• ë‹¨ê³„ì˜ 'ì¶œë ¥'ì„ ë’· ë‹¨ê³„ì˜ 'ì…ë ¥'ìœ¼ë¡œ ë®ì–´ì“´ë‹¤.
# 2. í•˜ì§€ë§Œ ë’· ë‹¨ê³„ì—ì„œ ì›ë˜ ì§ˆë¬¸ê³¼ ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ë™ì‹œì— í•„ìš”í•˜ë‹¤ë©´?
# 3. ë°ì´í„°ë¥¼ ìƒì–´ë²„ë¦¬ì§€ ì•Šê³  ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ëˆ„ì (Assign)í•˜ê±°ë‚˜,
#    í•„ìš”í•œ ê²ƒë§Œ ì„ íƒ(Pick)í•´ì„œ ì „ë‹¬í•˜ëŠ” ê¸°ìˆ ì´ í•„ìš”í•˜ë‹¤.
#
# í™œìš© ì‚¬ë¡€:
# - RAG(ê²€ìƒ‰ ì¦ê°• ìƒì„±): ì§ˆë¬¸(question)ì„ ìœ ì§€í•˜ë©´ì„œ ê²€ìƒ‰ ê²°ê³¼(context)ë¥¼ ì¶”ê°€í•  ë•Œ.
# - ëŒ€í™” ê¸°ë¡(History): ì‚¬ìš©ì ì…ë ¥ì— ëŒ€í™” ê¸°ë¡(chat_history)ì„ ë§ë¶™ì—¬ í”„ë¡¬í”„íŠ¸ë¡œ ë³´ë‚¼ ë•Œ.
# - API ì‘ë‹µ ì²˜ë¦¬: ë³µì¡í•œ JSON ê²°ê³¼ì—ì„œ íŠ¹ì • í•„ë“œ(ì˜ˆ: answer)ë§Œ ë½‘ì•„ì„œ í´ë¼ì´ì–¸íŠ¸ì— ì¤„ ë•Œ.
# ============================================================================

# load env
load_dotenv()

# LLM & Parser
llm = ChatOpenAI(model="gpt-4o-mini")
output_parser = StrOutputParser()

# ğŸ” ë””ë²„ê¹… í—¬í¼: ì²´ì¸ ì¤‘ê°„ ë°ì´í„° í™•ì¸ìš©
def debug_step(step_name):
    def _print_data(x):
        print(f"\nğŸ‘€ [Debug] {step_name}")
        print(f"   ë°ì´í„°: {x}")
        return x
    return RunnableLambda(_print_data)

# ------------------------------------------------------------------
# 1. ê¸°ë³¸: RunnablePassthrough.assign() - "ë°ì´í„° ëˆ„ì í•˜ê¸°"
# ------------------------------------------------------------------
# ì„¤ëª…: ì…ë ¥ë°›ì€ ë”•ì…”ë„ˆë¦¬ì— ìƒˆë¡œìš´ í‚¤-ê°’ì„ ì¶”ê°€í•©ë‹ˆë‹¤. (ê¸°ì¡´ ë°ì´í„° ì‚­ì œ X)
# íë¦„: {key1: val1}  --->  assign(key2=func)  --->  {key1: val1, key2: val2}

print("="*60)
print("1. RunnablePassthrough.assign() - ë°ì´í„° ëˆ„ì  ì˜ˆì œ")
print("="*60)

def get_user_info(input_dict):
    # DBì—ì„œ ì‚¬ìš©ì ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ê²ƒì„ ê°€ì •
    return "VIP_Member" if input_dict.get("user_id") == "user_123" else "Standard_Member"

# ì²´ì¸ êµ¬ì„±
# 1. ì…ë ¥: {"user_id": "...", "query": "..."}
# 2. assign: ì…ë ¥ê°’ì€ ê·¸ëŒ€ë¡œ ë‘ê³ , 'user_grade'ë¼ëŠ” í•„ë“œë§Œ ê³„ì‚°í•´ì„œ ì¶”ê°€í•¨
chain_with_assign = (
    debug_step("1. ì´ˆê¸° ì…ë ¥ ë°ì´í„°")
    | RunnablePassthrough.assign(user_grade=get_user_info)
    | debug_step("2. assign('user_grade') ì‹¤í–‰ í›„ ë°ì´í„°")
)

# ì‹¤í–‰
print(">>> ì‹¤í–‰ ê²°ê³¼:")
result = chain_with_assign.invoke({"user_id": "user_123", "query": "í™˜ë¶ˆ ê·œì •ì´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?"})
# ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ì— user_id, queryëŠ” ê·¸ëŒ€ë¡œ ìˆê³  user_gradeê°€ ì¶”ê°€ë¨


# ------------------------------------------------------------------
# 2. ì‹¤ì „ ì‘ìš©: RAG íŒ¨í„´ (ì§ˆë¬¸ + ë¬¸ì„œ ë™ì‹œì— ì „ë‹¬í•˜ê¸°)
# ------------------------------------------------------------------
# ê°€ì¥ ë§ì´ ì“°ëŠ” íŒ¨í„´ì…ë‹ˆë‹¤.
# í”„ë¡¬í”„íŠ¸ì—ëŠ” {question}ê³¼ {context} ë‘ ê°€ì§€ ë³€ìˆ˜ê°€ í•„ìš”í•˜ë‹¤.
# ë¦¬íŠ¸ë¦¬ë²„(ê²€ìƒ‰ê¸°)ëŠ” {context}ë§Œ ì°¾ì•„ì¤€ë‹¤. ì´ë•Œ {question}ì„ ìƒì–´ë²„ë¦¬ë©´ ì•ˆë¨  â†’ ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼ì— ì§ˆë¬¸ì´ í¬í•¨ë˜ì–´ì•¼ í•¨.

print("\n" + "="*60)
print("2. ì‹¤ì „ ì‘ìš©: RAG íŒ¨í„´ (ì§ˆë¬¸ ë³´ì¡´ + ë¬¸ì„œ ì¶”ê°€)")
print("="*60)

# ê°€ìƒì˜ ë¬¸ì„œ ê²€ìƒ‰ í•¨ìˆ˜ (Retriever)
def fake_retriever(query):
    print(f"   (ì‹œìŠ¤í…œ: '{query}'ì— ëŒ€í•œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤...)")
    return "ë­ì²´ì¸(LangChain)ì€ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ ìœ„í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤."

# í”„ë¡¬í”„íŠ¸: ë³€ìˆ˜ 2ê°œ í•„ìš”
rag_prompt = ChatPromptTemplate.from_template(
    "ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.\n\n[ë¬¸ì„œ]: {context}\n\n[ì§ˆë¬¸]: {question}"
)

# RAG ì²´ì¸ êµ¬ì„±
# input: {"question": "LangChainì´ ë­ì•¼?"}
# step 1: assign(context=...) -> questionì€ ìœ ì§€í•˜ê³ , context í‚¤ì— ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€
#         ê²°ê³¼: {"question": "...", "context": "ê²€ìƒ‰ëœ ë‚´ìš©"}
# step 2: prompt -> ì™„ì„±ëœ ë”•ì…”ë„ˆë¦¬ê°€ í”„ë¡¬í”„íŠ¸ì˜ {question}, {context}ì— ë§¤í•‘ë¨
rag_chain = (
    debug_step("1. ì´ˆê¸° ì§ˆë¬¸ ë°ì´í„°")
    | RunnablePassthrough.assign(context=lambda x: fake_retriever(x["question"]))
    | debug_step("2. assign('context') ì‹¤í–‰ í›„ (í”„ë¡¬í”„íŠ¸ ì…ë ¥ê°’)")
    | rag_prompt 
    | llm 
    | output_parser
)

# ì‹¤í–‰
query = "LangChainì´ ë­ì•¼?"
print(f"ì§ˆë¬¸: {query}")
rag_result = rag_chain.invoke({"question": query})
print(f"ë‹µë³€: {rag_result}")


# ------------------------------------------------------------------
# 3. RunnablePick: í•„ìš”í•œ ë°ì´í„°ë§Œ ë½‘ê¸°
# ------------------------------------------------------------------
# ì²´ì¸ ì¤‘ê°„ì´ë‚˜ ëì—ì„œ ë„ˆë¬´ ë§ì€ ì •ë³´ê°€ ë”•ì…”ë„ˆë¦¬ì— ìŒ“ì—¬ìˆì„ ë•Œ,
# ì›í•˜ëŠ” í‚¤ ê°’ë§Œ ì¶”ì¶œí•´ì„œ ë‹¤ìŒ ë‹¨ê³„ë¡œ ë„˜ê¸°ê±°ë‚˜ ìµœì¢… ì¶œë ¥.

print("\n" + "="*60)
print("3. RunnablePick - ë°ì´í„° ì¶”ì¶œ ì˜ˆì œ")
print("="*60)

# ê°€ìƒì˜ ë³µì¡í•œ API ì‘ë‹µ (ì´ì „ ë‹¨ê³„ì˜ ê²°ê³¼ë¼ê³  ê°€ì •)
complex_output = {
    "status": 200,
    "metadata": {"latency": 0.5, "tokens": 150},
    "content": {
        "answer": "ì„œìš¸ì…ë‹ˆë‹¤.",
        "sources": ["wiki", "news"]
    }
}

# ì²´ì¸: ì „ì²´ ë°ì´í„° -> 'content' ì¶”ì¶œ -> ê·¸ ì•ˆì—ì„œ 'answer' ì¶”ì¶œ
# ë”•ì…”ë„ˆë¦¬ ì ‘ê·¼ë²•: complex_output["content"]["answer"] ì™€ ê°™ìŒ
pick_chain = (
    debug_step("1. ì›ë³¸ ë°ì´í„°")
    | RunnablePick("content") 
    | debug_step("2. Pick('content') ì‹¤í–‰ í›„")
    | RunnablePick("answer")
    | debug_step("3. Pick('answer') ì‹¤í–‰ í›„ (ìµœì¢… ê²°ê³¼)")
)

# ì‹¤í–‰
# invokeì— ë“¤ì–´ê°€ëŠ” ê°’ì´ ìœ„ì—ì„œ ì •ì˜í•œ ë”•ì…”ë„ˆë¦¬ë¼ê³  ê°€ì •
result_pick = pick_chain.invoke(complex_output)
print(f"ì›ë³¸ ë°ì´í„° í‚¤: {complex_output.keys()}")
print(f"Pick ê²°ê³¼: {result_pick}")
